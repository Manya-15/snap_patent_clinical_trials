{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, quote\n",
    "import csv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class PubChemPatentScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.base_url = \"https://pubchem.ncbi.nlm.nih.gov\"\n",
    "    \n",
    "    def get_substance_sid(self, query):\n",
    "        \"\"\"Get SID for a substance by name.\"\"\"\n",
    "        search_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/substance/name/{quote(query)}/sids/JSON\"\n",
    "        try:\n",
    "            response = self.session.get(search_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            sid = data['IdentifierList']['SID'][0]\n",
    "            print(f\"Found SID: {sid}\")\n",
    "            return sid\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching SID for '{query}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_patents_from_web(self, query):\n",
    "        \"\"\"Scrape patents directly from the PubChem web interface.\"\"\"\n",
    "        print(f\"Scraping patents for '{query}' from web interface...\")\n",
    "        \n",
    "        # First get the SID\n",
    "        sid = self.get_substance_sid(query)\n",
    "        if not sid:\n",
    "            return []\n",
    "        \n",
    "        # Go to the substance page\n",
    "        substance_url = f\"https://pubchem.ncbi.nlm.nih.gov/substance/{sid}\"\n",
    "        print(f\"Accessing: {substance_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(substance_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Look for patent-related links or sections\n",
    "            patents = self.extract_patents_from_page(soup, sid)\n",
    "            \n",
    "            if patents:\n",
    "                return patents\n",
    "            \n",
    "            # If no patents found, try the patents-specific page\n",
    "            patents_url = f\"https://pubchem.ncbi.nlm.nih.gov/substance/{sid}#section=Patents\"\n",
    "            print(f\"Trying patents section: {patents_url}\")\n",
    "            \n",
    "            response = self.session.get(patents_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            patents = self.extract_patents_from_page(soup, sid)\n",
    "            return patents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping web page: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_patents_from_page(self, soup, sid):\n",
    "        \"\"\"Extract patent information from the parsed HTML.\"\"\"\n",
    "        patents = []\n",
    "        \n",
    "        # Look for various patent-related patterns\n",
    "        patent_patterns = [\n",
    "            r'US-\\d+[A-Z]*\\d*',  # US patents\n",
    "            r'EP-\\d+[A-Z]*\\d*',  # European patents\n",
    "            r'WO-\\d+[A-Z]*\\d*',  # World patents\n",
    "            r'JP-\\d+[A-Z]*\\d*',  # Japanese patents\n",
    "            r'CN-\\d+[A-Z]*\\d*',  # Chinese patents\n",
    "        ]\n",
    "        \n",
    "        # Search in all text content\n",
    "        page_text = soup.get_text()\n",
    "        for pattern in patent_patterns:\n",
    "            matches = re.findall(pattern, page_text)\n",
    "            patents.extend(matches)\n",
    "        \n",
    "        # Look for patent links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            text = link.get_text().strip()\n",
    "            \n",
    "            # Check if link contains patent information\n",
    "            for pattern in patent_patterns:\n",
    "                if re.search(pattern, href) or re.search(pattern, text):\n",
    "                    match = re.search(pattern, href + ' ' + text)\n",
    "                    if match:\n",
    "                        patents.append(match.group())\n",
    "        \n",
    "        # Remove duplicates and clean up\n",
    "        patents = list(set(patents))\n",
    "        print(f\"Found {len(patents)} patents from web scraping\")\n",
    "        \n",
    "        return patents\n",
    "    \n",
    "    def download_patent_data_directly(self, query):\n",
    "        \"\"\"Attempt to download patent data using PubChem's download functionality.\"\"\"\n",
    "        print(f\"Attempting direct patent data download for '{query}'...\")\n",
    "        \n",
    "        sid = self.get_substance_sid(query)\n",
    "        if not sid:\n",
    "            return []\n",
    "        \n",
    "        # Try different download URLs that might contain patent data\n",
    "        download_urls = [\n",
    "            f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/{sid}/JSON?heading=Patents\",\n",
    "            f\"https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&outfmt=csv&query={{\\\"download\\\":\\\"*\\\",\\\"collection\\\":\\\"substance\\\",\\\"where\\\":{{\\\"ands\\\":[{{\\\"sid\\\":\\\"{sid}\\\"}}]}},\\\"order\\\":[\\\"relevancescore,desc\\\"],\\\"start\\\":1,\\\"limit\\\":10000}}\",\n",
    "            f\"https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&outfmt=json&query={{\\\"download\\\":\\\"*\\\",\\\"collection\\\":\\\"substance\\\",\\\"where\\\":{{\\\"ands\\\":[{{\\\"sid\\\":\\\"{sid}\\\"}}]}},\\\"order\\\":[\\\"relevancescore,desc\\\"],\\\"start\\\":1,\\\"limit\\\":10000}}\"\n",
    "        ]\n",
    "        \n",
    "        for url in download_urls:\n",
    "            try:\n",
    "                print(f\"Trying download URL: {url[:100]}...\")\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Try to parse as JSON first\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    print(f\"Successfully got JSON data: {len(str(data))} characters\")\n",
    "                    patents = self.extract_patents_from_json(data)\n",
    "                    if patents:\n",
    "                        return patents\n",
    "                except:\n",
    "                    # Try as CSV\n",
    "                    if 'csv' in url:\n",
    "                        print(f\"Got CSV data: {len(response.text)} characters\")\n",
    "                        patents = self.extract_patents_from_csv(response.text)\n",
    "                        if patents:\n",
    "                            return patents\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with download URL: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def extract_patents_from_json(self, data):\n",
    "        \"\"\"Extract patents from JSON data.\"\"\"\n",
    "        patents = []\n",
    "        \n",
    "        def recursive_search(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                for key, value in obj.items():\n",
    "                    if 'patent' in key.lower():\n",
    "                        if isinstance(value, str):\n",
    "                            patents.append(value)\n",
    "                        elif isinstance(value, list):\n",
    "                            patents.extend([str(v) for v in value])\n",
    "                    recursive_search(value)\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    recursive_search(item)\n",
    "        \n",
    "        recursive_search(data)\n",
    "        return list(set(patents))\n",
    "    \n",
    "    def extract_patents_from_csv(self, csv_text):\n",
    "        \"\"\"Extract patents from CSV data.\"\"\"\n",
    "        patents = []\n",
    "        lines = csv_text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Look for patent patterns in each line\n",
    "            patent_patterns = [\n",
    "                r'US-\\d+[A-Z]*\\d*',\n",
    "                r'EP-\\d+[A-Z]*\\d*',\n",
    "                r'WO-\\d+[A-Z]*\\d*',\n",
    "                r'JP-\\d+[A-Z]*\\d*',\n",
    "                r'CN-\\d+[A-Z]*\\d*',\n",
    "            ]\n",
    "            \n",
    "            for pattern in patent_patterns:\n",
    "                matches = re.findall(pattern, line)\n",
    "                patents.extend(matches)\n",
    "        \n",
    "        return list(set(patents))\n",
    "    \n",
    "    def search_patents_comprehensive(self, query):\n",
    "        \"\"\"Comprehensive patent search using multiple methods.\"\"\"\n",
    "        all_patents = []\n",
    "        \n",
    "        # Method 1: Web scraping\n",
    "        print(\"\\n=== Method 1: Web Scraping ===\")\n",
    "        web_patents = self.scrape_patents_from_web(query)\n",
    "        all_patents.extend(web_patents)\n",
    "        \n",
    "        # Method 2: Direct download attempts\n",
    "        print(\"\\n=== Method 2: Direct Download ===\")\n",
    "        download_patents = self.download_patent_data_directly(query)\n",
    "        all_patents.extend(download_patents)\n",
    "        \n",
    "        # Method 3: Search through related compounds/substances\n",
    "        print(\"\\n=== Method 3: Related Searches ===\")\n",
    "        related_patents = self.search_related_entries(query)\n",
    "        all_patents.extend(related_patents)\n",
    "        \n",
    "        # Clean and deduplicate\n",
    "        unique_patents = list(set(all_patents))\n",
    "        return unique_patents\n",
    "    \n",
    "    def search_related_entries(self, query):\n",
    "        \"\"\"Search for patents in related compounds or substances.\"\"\"\n",
    "        patents = []\n",
    "        \n",
    "        try:\n",
    "            # Try searching for compounds with similar names\n",
    "            search_variants = [\n",
    "                query,\n",
    "                query.replace('-', ' '),\n",
    "                query.replace(' ', '-'),\n",
    "                query.lower(),\n",
    "                query.upper()\n",
    "            ]\n",
    "            \n",
    "            for variant in search_variants[:2]:  # Limit to avoid too many requests\n",
    "                try:\n",
    "                    # Search for compounds\n",
    "                    compound_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{quote(variant)}/cids/JSON\"\n",
    "                    response = self.session.get(compound_url)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        cids = data.get('IdentifierList', {}).get('CID', [])\n",
    "                        \n",
    "                        for cid in cids[:3]:  # Check first few CIDs\n",
    "                            compound_patents = self.get_patents_for_cid(cid)\n",
    "                            patents.extend(compound_patents)\n",
    "                            time.sleep(0.5)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error searching variant '{variant}': {e}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in related searches: {e}\")\n",
    "        \n",
    "        return patents\n",
    "    \n",
    "    def get_patents_for_cid(self, cid):\n",
    "        \"\"\"Get patents for a specific compound CID.\"\"\"\n",
    "        try:\n",
    "            # Try to access the compound's web page\n",
    "            compound_url = f\"https://pubchem.ncbi.nlm.nih.gov/compound/{cid}\"\n",
    "            response = self.session.get(compound_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            patents = self.extract_patents_from_page(soup, cid)\n",
    "            \n",
    "            if patents:\n",
    "                print(f\"Found {len(patents)} patents for CID {cid}\")\n",
    "            \n",
    "            return patents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting patents for CID {cid}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_patent_dataframe(self, patents, query):\n",
    "        \"\"\"Create a DataFrame similar to your manual download format.\"\"\"\n",
    "        if not patents:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create a basic DataFrame with the patent numbers\n",
    "        df_data = []\n",
    "        \n",
    "        for i, patent in enumerate(patents):\n",
    "            row = {\n",
    "                'publicationnumber': patent,\n",
    "                'cids': '',  # Would need additional API calls to populate\n",
    "                'sids': '',  # Would need additional API calls to populate  \n",
    "                'refsids': '',\n",
    "                'title': f'Patent related to {query}',  # Placeholder\n",
    "                'abstract': '',  # Would need patent API to get full details\n",
    "                'prioritydate': '',\n",
    "                'grantdate': '',\n",
    "                'inventors': '',\n",
    "                'assignees': '',\n",
    "                'classification': '',\n",
    "                'family': '',\n",
    "                'aids': '',\n",
    "                'geneids': '',\n",
    "                'protacxns': '',\n",
    "                'taxids': '',\n",
    "                'anatomyids': ''\n",
    "            }\n",
    "            df_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        return df\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    scraper = PubChemPatentScraper()\n",
    "    query = \"alpha-2-macroglobulin\"\n",
    "    \n",
    "    print(f\"Searching for patents related to '{query}'...\")\n",
    "    patents = scraper.search_patents_comprehensive(query)\n",
    "    \n",
    "    if patents:\n",
    "        print(f\"\\n=== RESULTS ===\")\n",
    "        print(f\"Found {len(patents)} unique patents:\")\n",
    "        \n",
    "        for i, patent in enumerate(patents, 1):\n",
    "            print(f\"{i:3d}. {patent}\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = scraper.create_patent_dataframe(patents, query)\n",
    "        \n",
    "        # Save to CSV\n",
    "        filename = f'patents_{query.replace(\" \", \"_\")}_scraped.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nResults saved to '{filename}'\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n=== NO PATENTS FOUND ===\")\n",
    "        print(\"Consider:\")\n",
    "        print(\"1. The substance might not have associated patents\")\n",
    "        print(\"2. Patents might be behind authentication\")\n",
    "        print(\"3. Different search terms might be needed\")\n",
    "        print(\"4. Manual download from PubChem might be required\")\n",
    "        \n",
    "        print(f\"\\nTry manually visiting:\")\n",
    "        sid = scraper.get_substance_sid(query)\n",
    "        if sid:\n",
    "            print(f\"https://pubchem.ncbi.nlm.nih.gov/substance/{sid}#section=Patents\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
