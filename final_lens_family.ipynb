{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d1f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fetching 'Expand by Simple Family' for: 'Cytonics CORP' ---\n",
      "Result: 88 rows, 37 columns.\n",
      "✅ Successfully saved simple expanded data to 'lens-export-simple-expanded.csv'\n",
      "\n",
      "--- Fetching 'Group by Simple Family' for: 'Cytonics CORP' ---\n",
      "Result: 10 rows, 37 columns.\n",
      "✅ Successfully saved simple grouped data to 'lens-export-simple-grouped.csv'\n",
      "\n",
      "--- Fetching 'Expand by Extended Family' for: 'Cytonics CORP' ---\n",
      "Result: 90 rows, 37 columns.\n",
      "✅ Successfully saved extended family data to 'lens-export-extended-family-expanded.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from urllib.parse import quote # <-- Import the 'quote' function\n",
    "\n",
    "# This is the base JSON payload, used by all three functions.\n",
    "BASE_PAYLOAD = {\n",
    "    \"track_total_hits\": True,\n",
    "    \"size\": 2000,\n",
    "    \"from\": 0,\n",
    "    \"_source\": [\"abstract\", \"agent\", \"applicant\", \"application_reference.date\", \"application_reference.doc_number\", \"application_reference.kind\", \"application_reference.jurisdiction\", \"assistant_examiner\", \"cited_by.patent_count\", \"cited_by.patent.lens_id\", \"cites_patent\", \"claim\", \"class_cpc\", \"class_ipcr\", \"class_national\", \"date_published\", \"doc_key\", \"doc_number\", \"earliest_priority_claim_date\", \"examiner\", \"family.extended.id\", \"family.extended.size\", \"family.simple\", \"family.simple.id\", \"family.simple.size\", \"has_abstract\", \"has_claim\", \"has_description\", \"has_docdb\", \"has_examiner\", \"has_full_text\", \"has_title\", \"inventor\", \"jurisdiction\", \"kind\", \"legal_status\", \"lens_internal.legacy_pub_key\", \"npl_citation_count\", \"owner_all\", \"patent_citation_count\", \"primary_examiner\", \"priority_claim\", \"publication_type\", \"record_lens_id\", \"reference_cited.npl_count\", \"reference_cited.npl_resolved_count\", \"reference_cited.patent_count\", \"reference_cited.patent.lens_id\", \"sequence\", \"title\"],\n",
    "    \"highlight\": {\n",
    "        \"type\": \"plain\",\n",
    "        \"pre_tags\": [\"<span class=\\\"highlight\\\">\"],\n",
    "        \"post_tags\": [\"</span>\"],\n",
    "        \"fields\": {\n",
    "            \"title\": {\"fragment_size\": 500},\n",
    "            \"fulltext\": {},\n",
    "            \"claim\": {},\n",
    "            \"description\": {},\n",
    "            \"abstract\": {}\n",
    "        },\n",
    "        \"number_of_fragments\": 3\n",
    "    },\n",
    "    \"sort\": [{\"_score\": {\"order\": \"desc\"}}],\n",
    "    \"sortField\": \"_score\",\n",
    "    \"sortOrder\": \"DESC\",\n",
    "    \"format\": \"CSV\",\n",
    "    \"fields\": [\"JURISDICTION\", \"KIND\", \"DISPLAY_KEY\", \"LENS_ID\", \"PUBLICATION_DATE\", \"PUBLICATION_YEAR\", \"APPLICATION_NUMBER\", \"APPLICATION_DATE\", \"PRIORITY_NUMBERS\", \"EARLIEST_PRIORITY_DATE\", \"TITLE\", \"ABSTRACT\", \"APPLICANTS\", \"INVENTORS\", \"OWNERS\", \"URL\", \"PUBLICATION_TYPE\", \"HAS_FULL_TEXT\", \"CITES_PATENT_COUNT\", \"CITED_BY_PATENT_COUNT\", \"SIMPLE_FAMILY_SIZE\", \"SIMPLE_FAMILY_MEMBER_LENS_IDS\", \"SIMPLE_FAMILY_MEMBER_JURISDICTIONS\", \"EXTENDED_FAMILY_SIZE\", \"EXTENDED_FAMILY_MEMBER_LENS_IDS\", \"EXTENDED_FAMILY_MEMBER_JURISDICTIONS\", \"SEQUENCE_COUNT\", \"CPC_CLASSIFICATIONS\", \"IPCR_CLASSIFICATIONS\", \"US_CLASSIFICATIONS\", \"NPL_CITATION_COUNT\", \"NPL_RESOLVED_CITATION_COUNT\", \"NPL_RESOLVED_LENS_IDS\", \"NPL_RESOLVED_EXTERNAL_IDS\", \"NPL_CITATIONS\", \"PATENT_STATUS\"],\n",
    "    \"filename\": \"lens-export\",\n",
    "    \"async\": False\n",
    "}\n",
    "\n",
    "def export_expand_by_simple_family(search_term: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches patent data expanded by simple family.\n",
    "    The search_term is URL-encoded to handle spaces and special characters.\n",
    "    \"\"\"\n",
    "    print(f\"--- Fetching 'Expand by Simple Family' for: '{search_term}' ---\")\n",
    "    # --- URL Encoding Added Here ---\n",
    "    encoded_term = quote(search_term)\n",
    "    url = f\"https://www.lens.org/lens/export/patent?q={encoded_term}&st=true&e=true&f=false&l=en\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=BASE_PAYLOAD, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(io.StringIO(response.text))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def export_group_by_simple_family(search_term: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches patent data grouped by simple family.\n",
    "    The search_term is URL-encoded to handle spaces and special characters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Fetching 'Group by Simple Family' for: '{search_term}' ---\")\n",
    "    # --- URL Encoding Added Here ---\n",
    "    encoded_term = quote(search_term)\n",
    "    url = f\"https://www.lens.org/lens/export/patent?q={encoded_term}&st=true&e=false&f=true&l=en\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=BASE_PAYLOAD, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(io.StringIO(response.text))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def export_expand_by_extended_family(search_term: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches patent data expanded by extended family.\n",
    "    The search_term is URL-encoded to handle spaces and special characters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Fetching 'Expand by Extended Family' for: '{search_term}' ---\")\n",
    "    # --- URL Encoding Added Here ---\n",
    "    encoded_term = quote(search_term)\n",
    "    url = f\"https://www.lens.org/lens/export/patent?q={encoded_term}&st=true&e=false&expandByExtendedFamily=true&f=false&l=en\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=BASE_PAYLOAD, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(io.StringIO(response.text))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"Cytonics CORP\"\n",
    "    \n",
    "    # Create a safe filename from the search query\n",
    "    # safe_filename_part = search_query.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "    # 1. Get and Save the \"Expand by Simple Family\" results\n",
    "    df_expanded_simple = export_expand_by_simple_family(search_query)\n",
    "    if df_expanded_simple is not None:\n",
    "        print(f\"Result: {df_expanded_simple.shape[0]} rows, {df_expanded_simple.shape[1]} columns.\")\n",
    "        expanded_filename = f\"lens-export-simple-expanded.csv\"\n",
    "        df_expanded_simple.to_csv(expanded_filename, index=False)\n",
    "        print(f\"✅ Successfully saved simple expanded data to '{expanded_filename}'\")\n",
    "    \n",
    "    # 2. Get and Save the \"Group by Simple Family\" results\n",
    "    df_grouped_simple = export_group_by_simple_family(search_query)\n",
    "    if df_grouped_simple is not None:\n",
    "        print(f\"Result: {df_grouped_simple.shape[0]} rows, {df_grouped_simple.shape[1]} columns.\")\n",
    "        grouped_filename = f\"lens-export-simple-grouped.csv\"\n",
    "        df_grouped_simple.to_csv(grouped_filename, index=False)\n",
    "        print(f\"✅ Successfully saved simple grouped data to '{grouped_filename}'\")\n",
    "\n",
    "    # 3. Get and Save the \"Expand by Extended Family\" results\n",
    "    df_expanded_extended = export_expand_by_extended_family(search_query)\n",
    "    if df_expanded_extended is not None:\n",
    "        print(f\"Result: {df_expanded_extended.shape[0]} rows, {df_expanded_extended.shape[1]} columns.\")\n",
    "        extended_filename = f\"lens-export-extended-family-expanded.csv\"\n",
    "        df_expanded_extended.to_csv(extended_filename, index=False)\n",
    "        print(f\"✅ Successfully saved extended family data to '{extended_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8f1d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML SELF CODES\\snaplife\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading patent data from CSV ---\n",
      "Loaded 7 patents to process.\n",
      "\n",
      "--- Step 2: Processing patents in batches to filter and score relevance to 'Cytonics CORP' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches (Filter & Score): 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Found and scored 5 biomedical patents.\n",
      "\n",
      "--- Step 3: Sorting results and selecting the top 20 patents ---\n",
      "\n",
      "--- Final Results ---\n",
      "               Lens ID                                              Title  \\\n",
      "0  014-920-934-512-545  SYSTEMS, COMPOSITIONS, AND METHODS FOR TRANSPL...   \n",
      "1  103-948-931-946-410  Therapeutic variant alpha-2-macroglobulin comp...   \n",
      "2  176-912-377-055-867  Systems, compositions, and methods for transpl...   \n",
      "3  086-963-205-553-664  Method for diagnosing and treating acute joint...   \n",
      "4  103-256-101-079-605  METHODS FOR DIAGNOSING AND TREATING PAIN IN TH...   \n",
      "\n",
      "                                            Abstract  relevance_score  \n",
      "0  Systems and methods for purification and conce...                6  \n",
      "1  A2M polypeptide compositions containing a non-...                6  \n",
      "2  Systems and methods for purification and conce...                6  \n",
      "3  The present invention provides methods, reagen...                6  \n",
      "4  The present invention provides methods, reagen...                6  \n",
      "\n",
      "✅ Success! Top 20 ranked patents saved to 'filtered_relevant_patents.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"KEY\"  # Replace with your actual key\n",
    "\n",
    "# The original search query used to generate the CSV. This is crucial for the ranking step.\n",
    "SEARCH_QUERY = \"Cytonics CORP\" \n",
    "\n",
    "# How many of the most relevant patents you want in the final output.\n",
    "K_MOST_RELEVANT = 20\n",
    "\n",
    "# --- Step 1: Load the data ---\n",
    "print(\"--- Step 1: Loading patent data from CSV ---\")\n",
    "try:\n",
    "    df = pd.read_csv(\"lens-export-simple-grouped.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'lens-export-simple-grouped.csv' not found. Please run the previous script first.\")\n",
    "    exit()\n",
    "\n",
    "# Clean column names and prepare data\n",
    "df.columns = df.columns.str.strip()\n",
    "df_source = df[[\"Lens ID\", \"Title\", \"Abstract\"]].dropna().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df_source)} patents to process.\")\n",
    "\n",
    "# --- Step 2: Single-Pass Filtering and Scoring ---\n",
    "print(f\"\\n--- Step 2: Processing patents in batches to filter and score relevance to '{SEARCH_QUERY}' ---\")\n",
    "\n",
    "def format_for_prompt(df_batch):\n",
    "    \"\"\"Formats a DataFrame batch into a list of dictionaries for the prompt.\"\"\"\n",
    "    return df_batch.to_dict(orient='records')\n",
    "\n",
    "# New, combined prompt for filtering AND scoring in one go\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a highly efficient patent analyst specializing in biotech and pharmaceuticals.\n",
    "Your task is to analyze a list of patents based on a given SEARCH QUERY.\n",
    "\n",
    "Follow these two steps for each patent in the list:\n",
    "1.  **Filter**: First, determine if the patent is relevant to the medical, pharmaceutical, or biotech fields. If it is NOT, ignore it completely.\n",
    "2.  **Score**: If the patent IS biomedical, assign a relevance score from 1 (low relevance) to 10 (high relevance) based on how closely its title and abstract match the SEARCH QUERY and for humans.\n",
    "\n",
    "Return a JSON list of objects. Each object must contain the 'lens_id' and 'relevance_score'.\n",
    "Only include patents in your output that you identified as biomedical.\n",
    "\n",
    "SEARCH QUERY: \"{search_query}\"\n",
    "\n",
    "PATENT DATA:\n",
    "{examples}\n",
    "\"\"\")\n",
    "\n",
    "# Setup LangChain components\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "parser = JsonOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Run the single-pass process\n",
    "batch_size = 40  # You can adjust this based on performance\n",
    "all_scored_patents = []\n",
    "\n",
    "for i in tqdm(range(0, len(df_source), batch_size), desc=\"Processing Batches (Filter & Score)\"):\n",
    "    batch_df = df_source.iloc[i:i+batch_size]\n",
    "    examples = format_for_prompt(batch_df)\n",
    "\n",
    "    try:\n",
    "        # The result will be a list of scored patents, e.g., [{'lens_id': '...', 'relevance_score': 8}, ...]\n",
    "        scored_batch = chain.invoke({\"search_query\": SEARCH_QUERY, \"examples\": examples})\n",
    "        if scored_batch:  # Ensure the result is not empty\n",
    "            all_scored_patents.extend(scored_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Error processing a batch, skipping. Details: {e}\")\n",
    "\n",
    "print(f\"\\nProcessing complete. Found and scored {len(all_scored_patents)} biomedical patents.\")\n",
    "\n",
    "# --- Step 3: Rank Locally and Save the Top K Results ---\n",
    "if all_scored_patents:\n",
    "    print(f\"\\n--- Step 3: Sorting results and selecting the top {K_MOST_RELEVANT} patents ---\")\n",
    "\n",
    "    # Convert the list of scored patents into a DataFrame\n",
    "    scored_df = pd.DataFrame(all_scored_patents)\n",
    "\n",
    "    # Sort by relevance_score in descending order to find the best matches\n",
    "    ranked_df = scored_df.sort_values(by='relevance_score', ascending=False)\n",
    "    \n",
    "    # Get the Lens IDs of the top K patents\n",
    "    top_k_ids = ranked_df.head(K_MOST_RELEVANT)['lens_id'].tolist()\n",
    "    \n",
    "    # Create the final output DataFrame by merging with original data to get all info\n",
    "    final_df = df_source[df_source['Lens ID'].isin(top_k_ids)].copy()\n",
    "    \n",
    "    # Add the relevance scores to the final output\n",
    "    final_df = final_df.merge(ranked_df, left_on='Lens ID', right_on='lens_id', how='left')\n",
    "    \n",
    "    # Sort the final DataFrame according to the rank and clean up columns\n",
    "    final_df['Lens ID'] = pd.Categorical(final_df['Lens ID'], categories=top_k_ids, ordered=True)\n",
    "    final_df = final_df.sort_values('Lens ID').drop(columns=['lens_id']).reset_index(drop=True)\n",
    "\n",
    "    # Save the result\n",
    "    output_filename = f\"filtered_relevant_patents.csv\"\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\n--- Final Results ---\")\n",
    "    print(final_df)\n",
    "    print(f\"\\n✅ Success! Top {K_MOST_RELEVANT} ranked patents saved to '{output_filename}'\")\n",
    "else:\n",
    "    print(\"\\nNo biomedical patents were found during processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691dc231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading local CSV files ---\n",
      "CSVs loaded and columns cleaned.\n",
      "\n",
      "--- Step 2: Preparing relevant Lens IDs ---\n",
      "Found 5 unique relevant Lens IDs.\n",
      "\n",
      "--- Step 3: Filtering group data for relevant patents ---\n",
      "Found 5 matching patents in the 'group-by' data.\n",
      "\n",
      "--- Step 4: Processing patents and their family members ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patents: 100%|██████████| 5/5 [00:00<00:00, 185.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Saving results to CSV ---\n",
      "✅ Saved: simple_patent_family.csv\n",
      "Total family member relationships traced: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from tqdm import tqdm # Optional: for a nice progress bar\n",
    "import time\n",
    "\n",
    "# ==============================================================================\n",
    "#  API FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "def fetch_lens_minimal_data(lens_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches key patent data from Lens.org for a given Lens ID,\n",
    "    with a retry mechanism for handling 429 rate-limiting errors.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.lens.org/lens/export/patent?q=lens_id%3A%22{lens_id}%22&st=true\"\n",
    "    \n",
    "    # --- Retry Logic ---\n",
    "    max_retries = 4\n",
    "    base_delay_seconds = 4 # The starting wait time\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, timeout=20)\n",
    "            \n",
    "            # Check for specific HTTP errors like 429\n",
    "            response.raise_for_status() \n",
    "\n",
    "            # --- Success Case ---\n",
    "            if response.text and len(response.text.splitlines()) > 1:\n",
    "                df = pd.read_csv(io.StringIO(response.text))\n",
    "                if not df.empty:\n",
    "                    row = df.iloc[0]\n",
    "                    return {\n",
    "                        \"Jurisdiction\": row.get(\"Jurisdiction\", \"N/A\"),\n",
    "                        \"Application Number\": row.get(\"Application Number\", \"N/A\"),\n",
    "                        \"Document Type\": row.get(\"Document Type\", \"N/A\"),\n",
    "                        \"Legal Status\": row.get(\"Legal Status\", \"N/A\")\n",
    "                    }\n",
    "            return {\n",
    "                \"Jurisdiction\": \"Not Found\", \"Application Number\": \"Not Found\",\n",
    "                \"Document Type\": \"Not Found\", \"Legal Status\": \"Not Found\"\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Check if the error is specifically 'Too Many Requests'\n",
    "            if e.response.status_code == 429:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Calculate wait time with exponential backoff + a little randomness\n",
    "                    wait_time = (base_delay_seconds ** (attempt + 1)) \n",
    "                    print(f\"⏳ Rate limit hit for {lens_id}. Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"❌ Max retries reached for {lens_id}. Giving up.\")\n",
    "                    return {\n",
    "                        \"Jurisdiction\": \"Error (429)\", \"Application Number\": \"Error (429)\",\n",
    "                        \"Document Type\": \"Error (429)\", \"Legal Status\": \"Error (429)\"\n",
    "                    }\n",
    "            else:\n",
    "                # Handle other HTTP errors (e.g., 404, 500)\n",
    "                print(f\"❌ Unhandled HTTP Error for {lens_id}: {e}\")\n",
    "                return {\n",
    "                    \"Jurisdiction\": f\"Error ({e.response.status_code})\", \"Application Number\": f\"Error ({e.response.status_code})\",\n",
    "                    \"Document Type\": f\"Error ({e.response.status_code})\", \"Legal Status\": f\"Error ({e.response.status_code})\"\n",
    "                }\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle network errors (e.g., timeout, connection error)\n",
    "            print(f\"❌ Network Error fetching data for {lens_id}: {e}\")\n",
    "            break # No point in retrying if network is down\n",
    "\n",
    "    # This is returned if the loop finishes due to a network error or other unhandled case\n",
    "    return {\n",
    "        \"Jurisdiction\": \"Error (API)\", \"Application Number\": \"Error (API)\",\n",
    "        \"Document Type\": \"Error (API)\", \"Legal Status\": \"Error (API)\"\n",
    "    }\n",
    "\n",
    "# ==============================================================================\n",
    "#  MAIN SCRIPT LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- Step 1: Loading local CSV files ---\")\n",
    "group_df = pd.read_csv(\"lens-export-simple-grouped.csv\")\n",
    "extend_df = pd.read_csv(\"lens-export-simple-expanded.csv\")\n",
    "relevant_df = pd.read_csv(\"filtered_relevant_patents.csv\")\n",
    "\n",
    "# Clean column names by removing leading/trailing whitespace\n",
    "group_df.columns = group_df.columns.str.strip()\n",
    "extend_df.columns = extend_df.columns.str.strip()\n",
    "print(\"CSVs loaded and columns cleaned.\")\n",
    "\n",
    "print(\"\\n--- Step 2: Preparing relevant Lens IDs ---\")\n",
    "relevant_lens_ids = set(relevant_df[\"Lens ID\"])\n",
    "print(f\"Found {len(relevant_lens_ids)} unique relevant Lens IDs.\")\n",
    "\n",
    "print(\"\\n--- Step 3: Filtering group data for relevant patents ---\")\n",
    "relevant_rows = group_df[group_df[\"Lens ID\"].isin(relevant_lens_ids)].copy()\n",
    "print(f\"Found {len(relevant_rows)} matching patents in the 'group-by' data.\")\n",
    "\n",
    "print(\"\\n--- Step 4: Processing patents and their family members ---\")\n",
    "records = []\n",
    "\n",
    "# Using tqdm for a progress bar\n",
    "for _, row in tqdm(relevant_rows.iterrows(), total=relevant_rows.shape[0], desc=\"Processing Patents\"):\n",
    "    lens_id = row[\"Lens ID\"]\n",
    "    jurisdiction = row.get(\"Jurisdiction\", \"N/A\")\n",
    "    app_number = row.get(\"Application Number\", \"N/A\")\n",
    "    doc_type = row.get(\"Document Type\", \"N/A\")\n",
    "    legal_status = row.get(\"Legal Status\", \"N/A\")\n",
    "    family_members_raw = row.get(\"Simple Family Members\", \"\")\n",
    "\n",
    "    # Family members are ;; separated\n",
    "    family_members = str(family_members_raw).split(\";;\") if pd.notna(family_members_raw) else []\n",
    "\n",
    "    for member_id in family_members:\n",
    "        member_id = member_id.strip()\n",
    "        if not member_id:\n",
    "            continue\n",
    "\n",
    "        # The parent patent itself is listed in its family members, so handle it.\n",
    "        if lens_id == member_id:\n",
    "            member_data = {\n",
    "                \"Jurisdiction\": \"no family\",\n",
    "                \"Application Number\": \"no family\",\n",
    "                \"Document Type\": \"no family\",\n",
    "                \"Legal Status\": \"no family\"\n",
    "            }\n",
    "        else:\n",
    "            # Try to find in local extend_df\n",
    "            match = extend_df[extend_df[\"Lens ID\"] == member_id]\n",
    "            if not match.empty:\n",
    "                member_data = {\n",
    "                    \"Jurisdiction\": match.iloc[0].get(\"Jurisdiction\", \"N/A\"),\n",
    "                    \"Application Number\": match.iloc[0].get(\"Application Number\", \"N/A\"),\n",
    "                    \"Document Type\": match.iloc[0].get(\"Document Type\", \"N/A\"),\n",
    "                    \"Legal Status\": match.iloc[0].get(\"Legal Status\", \"N/A\")\n",
    "                }\n",
    "            else:\n",
    "                # Not found locally — fetch via API\n",
    "                print(f\"🔎 Fetching from Lens API: {member_id}\")\n",
    "                member_data = fetch_lens_minimal_data(member_id)\n",
    "\n",
    "        # Save the relationship record\n",
    "        records.append({\n",
    "            \"Lens ID\": lens_id,\n",
    "            \"Jurisdiction\": jurisdiction,\n",
    "            \"Document Type\": doc_type,\n",
    "            \"Legal Status\": legal_status,\n",
    "            \"Application Number\": app_number,\n",
    "            \"Family Member Lens ID\": member_id,\n",
    "            \"Member Jurisdiction\": member_data[\"Jurisdiction\"],\n",
    "            \"Member Application Number\": member_data[\"Application Number\"],\n",
    "            \"Member Document Type\": member_data[\"Document Type\"],\n",
    "            \"Member Legal Status\": member_data[\"Legal Status\"]\n",
    "        })\n",
    "\n",
    "print(\"\\n--- Step 5: Saving results to CSV ---\")\n",
    "output_df = pd.DataFrame(records)\n",
    "output_df.to_csv(\"simple_patent_family.csv\", index=False)\n",
    "print(\"✅ Saved: simple_patent_family.csv\")\n",
    "print(f\"Total family member relationships traced: {len(output_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ab7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved grouped file to: lens_patent_data_simple_family.csv\n",
      "Total patents exported: 5\n",
      "✅ All family columns aligned (token counts match per row).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------- config ----------\n",
    "INFILE = \"simple_patent_family.csv\"\n",
    "OUTFILE = \"lens_patent_data_simple_family.csv\"\n",
    "SEP = \"||\"   # change to \"|\" or \"||\" as you want\n",
    "# ----------------------------\n",
    "\n",
    "# read (keep empty cells as empty strings)\n",
    "df = pd.read_csv(INFILE, sep=\",\", dtype=str, encoding=\"utf-8-sig\", keep_default_na=False)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# normalize 'no family' (case-insensitive, trim spaces)\n",
    "df = df.replace(to_replace=r'^\\s*no family\\s*$', value='', regex=True)\n",
    "\n",
    "# required columns\n",
    "required = [\n",
    "    \"Lens ID\", \"Jurisdiction\", \"Document Type\", \"Legal Status\", \"Application Number\",\n",
    "    \"Family Member Lens ID\", \"Member Jurisdiction\", \"Member Application Number\",\n",
    "    \"Member Document Type\", \"Member Legal Status\"\n",
    "]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns in input CSV: {missing}\\nAvailable columns: {df.columns.tolist()}\")\n",
    "\n",
    "pat_keys = [\"Lens ID\", \"Jurisdiction\", \"Document Type\", \"Legal Status\", \"Application Number\"]\n",
    "\n",
    "# Get all unique patents (so even patents with only self-rows will be included)\n",
    "unique_pats = df[pat_keys].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for _, pat in unique_pats.iterrows():\n",
    "    lens = pat[\"Lens ID\"]\n",
    "\n",
    "    # Select family rows for this patent BUT skip:\n",
    "    #  - rows where Family Member Lens ID is empty\n",
    "    #  - rows where Family Member Lens ID equals the parent Lens ID (self-reference)\n",
    "    mask = (\n",
    "        (df[\"Lens ID\"] == lens) &\n",
    "        (df[\"Family Member Lens ID\"].str.strip() != \"\") &\n",
    "        (df[\"Family Member Lens ID\"] != df[\"Lens ID\"])\n",
    "    )\n",
    "    fam = df.loc[mask, [\n",
    "        \"Family Member Lens ID\",\n",
    "        \"Member Jurisdiction\",\n",
    "        \"Member Application Number\",\n",
    "        \"Member Document Type\",\n",
    "        \"Member Legal Status\"\n",
    "    ]].copy()\n",
    "\n",
    "    # preserve original order (optional)\n",
    "    fam = fam.sort_index()\n",
    "\n",
    "    # fill NaNs with empty strings (shouldn't be needed given keep_default_na=False, but safe)\n",
    "    fam = fam.fillna(\"\")\n",
    "\n",
    "    if not fam.empty:\n",
    "        fm_lens = SEP.join(fam[\"Family Member Lens ID\"].astype(str))\n",
    "        fm_jur = SEP.join(fam[\"Member Jurisdiction\"].astype(str))\n",
    "        fm_app = SEP.join(fam[\"Member Application Number\"].astype(str))\n",
    "        fm_doc = SEP.join(fam[\"Member Document Type\"].astype(str))\n",
    "        fm_status = SEP.join(fam[\"Member Legal Status\"].astype(str))\n",
    "    else:\n",
    "        fm_lens = fm_jur = fm_app = fm_doc = fm_status = \"\"\n",
    "\n",
    "    row = {\n",
    "        **pat.to_dict(),\n",
    "        \"Family Member Lens IDs\": fm_lens,\n",
    "        \"Family Member Jurisdictions\": fm_jur,\n",
    "        \"Family Member Application Numbers\": fm_app,\n",
    "        \"Family Member Document Types\": fm_doc,\n",
    "        \"Family Member Legal Statuses\": fm_status\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "final_df = pd.DataFrame(rows, columns = pat_keys + [\n",
    "    \"Family Member Lens IDs\",\n",
    "    \"Family Member Jurisdictions\",\n",
    "    \"Family Member Application Numbers\",\n",
    "    \"Family Member Document Types\",\n",
    "    \"Family Member Legal Statuses\"\n",
    "])\n",
    "\n",
    "final_df.to_csv(OUTFILE, index=False)\n",
    "print(f\"✅ Saved grouped file to: {OUTFILE}\")\n",
    "print(f\"Total patents exported: {len(final_df)}\")\n",
    "\n",
    "# --- Optional alignment check (prints rows where token counts differ) ---\n",
    "fam_cols = [\n",
    "    \"Family Member Lens IDs\",\n",
    "    \"Family Member Jurisdictions\",\n",
    "    \"Family Member Application Numbers\",\n",
    "    \"Family Member Document Types\",\n",
    "    \"Family Member Legal Statuses\"\n",
    "]\n",
    "\n",
    "def token_count(cell):\n",
    "    s = str(cell)\n",
    "    if s == \"\" or s.strip() == \"\":\n",
    "        return 0\n",
    "    return len(s.split(SEP))\n",
    "\n",
    "bad_rows = []\n",
    "for i, r in final_df.iterrows():\n",
    "    counts = [token_count(r[c]) for c in fam_cols]\n",
    "    # If not all counts are equal (and not all zero) -> misalignment\n",
    "    nonzeros = [c for c in counts if c != 0]\n",
    "    if nonzeros and (len(set(nonzeros)) != 1):\n",
    "        bad_rows.append((i, counts))\n",
    "\n",
    "if bad_rows:\n",
    "    print(\"\\n⚠️ Alignment warnings (rows where family-column token counts disagree):\")\n",
    "    for i, counts in bad_rows[:20]:\n",
    "        print(f\" row {i}: counts = {counts}  (Lens ID = {final_df.at[i,'Lens ID']})\")\n",
    "    if len(bad_rows) > 20:\n",
    "        print(f\"...and {len(bad_rows)-20} more\")\n",
    "else:\n",
    "    print(\"✅ All family columns aligned (token counts match per row).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa09b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tree saved as: patent_family_tree_Cytonics_CORP.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyvis.network import Network\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(\"lens_patent_data_simple_family.csv\")\n",
    "\n",
    "# Replace NaN with default text to prevent render issues\n",
    "df.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Get first 20 unique application numbers\n",
    "unique_apps = df[\"Application Number\"].dropna().unique()[:20]\n",
    "\n",
    "# Create the network\n",
    "net = Network(height=\"800px\", width=\"100%\", bgcolor=\"#ffffff\", directed=True, notebook=False)\n",
    "\n",
    "# Add root node\n",
    "net.add_node(\"patent\", label=\"Patent Root\", shape='box', color='red')\n",
    "\n",
    "# Add first-layer nodes and family member nodes\n",
    "for app_no in unique_apps:\n",
    "    app_df = df[df[\"Application Number\"] == app_no]\n",
    "    meta = app_df.iloc[0]\n",
    "\n",
    "    # Add main application node\n",
    "    hover_text = f\"Jurisdiction: {meta['Jurisdiction']}<br>Type: {meta['Document Type']}<br>Status: {meta['Legal Status']}\"\n",
    "    net.add_node(app_no, label=app_no, title=hover_text, shape='ellipse', color='orange')\n",
    "    net.add_edge(\"patent\", app_no)\n",
    "\n",
    "    # Add family member nodes\n",
    "    member_apps = meta[\"Family Member Application Numbers\"]\n",
    "    member_jurisdictions = meta[\"Family Member Jurisdictions\"]\n",
    "    member_docs = meta[\"Family Member Document Types\"]\n",
    "    member_statuses = meta[\"Family Member Legal Statuses\"]\n",
    "\n",
    "    if member_apps != \"Unknown\" and member_apps != \"no family\":\n",
    "        member_apps_list = member_apps.split('||')\n",
    "        member_jurisdictions_list = member_jurisdictions.split('||')\n",
    "        member_docs_list = member_docs.split('||')\n",
    "        member_statuses_list = member_statuses.split('||')\n",
    "\n",
    "        # To handle cases where lists might have different lengths due to malformed data, we take the minimum length\n",
    "        min_length = min(len(member_apps_list), len(member_jurisdictions_list), len(member_docs_list), len(member_statuses_list))\n",
    "\n",
    "        for i in range(min_length):\n",
    "            member_app = member_apps_list[i]\n",
    "            member_jurisdiction = member_jurisdictions_list[i]\n",
    "            member_doc = member_docs_list[i]\n",
    "            member_status = member_statuses_list[i]\n",
    "\n",
    "            # Skip the main application itself if it's in the family list\n",
    "            if member_app == app_no:\n",
    "                continue\n",
    "\n",
    "            hover_text = f\"Jurisdiction: {member_jurisdiction}<br>Type: {member_doc}<br>Status: {member_status}\"\n",
    "            # Check if node already exists before adding\n",
    "            if member_app not in net.get_nodes():\n",
    "                net.add_node(member_app, label=member_app, title=hover_text, shape='ellipse', color='lightblue')\n",
    "            net.add_edge(app_no, member_app)\n",
    "\n",
    "# Write HTML manually\n",
    "net.write_html(\"patent_family_tree_Cytonics_CORP.html\")\n",
    "\n",
    "print(\"✅ Tree saved as: patent_family_tree_Cytonics_CORP.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfcd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
